{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pprint import pprint as print\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.word2vec import Word2Vec, Text8Corpus\n",
    "import numpy as np\n",
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prime prove fastText\n",
    "il modello creato Ã¨ stato creato con 1 epoca dal modello wikipedia, da migliorare e diminuire le parole, \n",
    "introducendo un dizionario di paroli comuni italiane (vedi come fa il professore)\n",
    "\n",
    "le funzioni seguenti servono per fare lo spell checker e non per predirre la parola sucessiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-02 19:14:33,938 : INFO : loading FastText object from WikiFast1tris.model\n",
      "2020-04-02 19:14:34,117 : INFO : loading wv recursively from WikiFast1tris.model.wv.* with mmap=None\n",
      "2020-04-02 19:14:34,118 : INFO : loading vectors from WikiFast1tris.model.wv.vectors.npy with mmap=None\n",
      "2020-04-02 19:14:34,200 : INFO : loading vectors_vocab from WikiFast1tris.model.wv.vectors_vocab.npy with mmap=None\n",
      "2020-04-02 19:14:34,272 : INFO : loading vectors_ngrams from WikiFast1tris.model.wv.vectors_ngrams.npy with mmap=None\n",
      "2020-04-02 19:14:36,332 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-04-02 19:14:36,334 : INFO : setting ignored attribute vectors_vocab_norm to None\n",
      "2020-04-02 19:14:36,334 : INFO : setting ignored attribute vectors_ngrams_norm to None\n",
      "2020-04-02 19:14:36,334 : INFO : setting ignored attribute buckets_word to None\n",
      "2020-04-02 19:14:36,335 : INFO : loading vocabulary recursively from WikiFast1tris.model.vocabulary.* with mmap=None\n",
      "2020-04-02 19:14:36,335 : INFO : loading trainables recursively from WikiFast1tris.model.trainables.* with mmap=None\n",
      "2020-04-02 19:14:36,336 : INFO : loading syn1neg from WikiFast1tris.model.trainables.syn1neg.npy with mmap=None\n",
      "2020-04-02 19:14:36,547 : INFO : loading vectors_vocab_lockf from WikiFast1tris.model.trainables.vectors_vocab_lockf.npy with mmap=None\n",
      "2020-04-02 19:14:36,664 : INFO : loading vectors_ngrams_lockf from WikiFast1tris.model.trainables.vectors_ngrams_lockf.npy with mmap=None\n",
      "2020-04-02 19:14:38,620 : INFO : loaded WikiFast1tris.model\n"
     ]
    }
   ],
   "source": [
    "model = FT_gensim.load('WikiFast1tris.model') #modello creato con fast text 1 epoca (da migliorare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/stefanoraimondousai/Documents/ReadingCourse/text_pulito.txt' #testo con chat marta\n",
    "sentences = datapath(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-02 19:14:40,585 : INFO : training model with 8 workers on 63937 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-04-02 19:14:40,595 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-04-02 19:14:40,599 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-04-02 19:14:40,599 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-04-02 19:14:40,600 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-04-02 19:14:40,602 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-02 19:14:40,602 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-02 19:14:40,603 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-02 19:14:40,604 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-02 19:14:40,605 : INFO : EPOCH - 1 : training on 66 raw words (45 effective words) took 0.0s, 3414 effective words/s\n",
      "2020-04-02 19:14:40,606 : WARNING : EPOCH - 1 : supplied example count (66) did not equal expected count (11036232)\n",
      "2020-04-02 19:14:40,611 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-04-02 19:14:40,612 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-04-02 19:14:40,612 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-04-02 19:14:40,613 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-04-02 19:14:40,614 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-02 19:14:40,615 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-02 19:14:40,616 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-02 19:14:40,616 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-02 19:14:40,617 : INFO : EPOCH - 2 : training on 66 raw words (50 effective words) took 0.0s, 8404 effective words/s\n",
      "2020-04-02 19:14:40,618 : WARNING : EPOCH - 2 : supplied example count (66) did not equal expected count (11036232)\n",
      "2020-04-02 19:14:40,622 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-04-02 19:14:40,623 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-04-02 19:14:40,624 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-04-02 19:14:40,625 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-04-02 19:14:40,625 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-02 19:14:40,626 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-02 19:14:40,627 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-02 19:14:40,627 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-02 19:14:40,628 : INFO : EPOCH - 3 : training on 66 raw words (46 effective words) took 0.0s, 8474 effective words/s\n",
      "2020-04-02 19:14:40,629 : WARNING : EPOCH - 3 : supplied example count (66) did not equal expected count (11036232)\n",
      "2020-04-02 19:14:40,635 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-04-02 19:14:40,636 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-04-02 19:14:40,636 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-04-02 19:14:40,637 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-04-02 19:14:40,638 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-02 19:14:40,638 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-02 19:14:40,639 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-02 19:14:40,640 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-02 19:14:40,640 : INFO : EPOCH - 4 : training on 66 raw words (44 effective words) took 0.0s, 7451 effective words/s\n",
      "2020-04-02 19:14:40,641 : WARNING : EPOCH - 4 : supplied example count (66) did not equal expected count (11036232)\n",
      "2020-04-02 19:14:40,646 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-04-02 19:14:40,647 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-04-02 19:14:40,647 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-04-02 19:14:40,648 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-04-02 19:14:40,649 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-02 19:14:40,649 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-02 19:14:40,650 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-02 19:14:40,651 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-02 19:14:40,651 : INFO : EPOCH - 5 : training on 66 raw words (45 effective words) took 0.0s, 7858 effective words/s\n",
      "2020-04-02 19:14:40,652 : WARNING : EPOCH - 5 : supplied example count (66) did not equal expected count (11036232)\n",
      "2020-04-02 19:14:40,652 : INFO : training on a 330 raw words (230 effective words) took 0.1s, 3572 effective words/s\n",
      "2020-04-02 19:14:40,653 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "model.train(sentences,epochs=5,total_examples=model.corpus_count, compute_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = list(model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict_output_word(model, context_words_list, topn=10):\n",
    "        from numpy import exp, dot, zeros, random, dtype, float32 as REAL,\\\n",
    "        uint32, seterr, array, uint8, vstack, fromstring, sqrt,\\\n",
    "        empty, sum as np_sum, ones, logaddexp, log, outer\n",
    "        from scipy.special import expit\n",
    "        from gensim import utils, matutils  # utility fnc for pickling, common scipy operations etc\n",
    "\n",
    "        if not model.negative:\n",
    "            raise RuntimeError(\n",
    "                \"We have currently only implemented predict_output_word for the negative sampling scheme, \"\n",
    "                \"so you need to have run word2vec with negative > 0 for this to work.\"\n",
    "            )\n",
    "\n",
    "        if not hasattr(model.wv, 'vectors') or not hasattr(model.trainables, 'syn1neg'):\n",
    "            raise RuntimeError(\"Parameters required for predicting the output words not found.\")\n",
    "\n",
    "        word_vocabs = [model.wv.vocab[w] for w in context_words_list if w in model.wv.vocab]\n",
    "        if not word_vocabs:\n",
    "            warnings.warn(\"All the input context words are out-of-vocabulary for the current model.\")\n",
    "            return None\n",
    "\n",
    "        word2_indices = [word.index for word in word_vocabs]\n",
    "\n",
    "        l1 = np_sum(model.wv.vectors[word2_indices], axis=0)\n",
    "        if word2_indices and model.cbow_mean:\n",
    "            l1 /= len(word2_indices)\n",
    "\n",
    "        # propagate hidden -> output and take softmax to get probabilities\n",
    "        prob_values = exp(dot(l1, model.trainables.syn1neg.T))\n",
    "        prob_values /= sum(prob_values)\n",
    "        top_indices = matutils.argsort(prob_values, topn=topn, reverse=True)\n",
    "        # returning the most probable output words with their probabilities\n",
    "        return [(model.wv.index2word[index1], prob_values[index1]) for index1 in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight(word, similar_word, similarity, constant = 0.5, min_weight=3):\n",
    "    min_similarity = similarity > constant   \n",
    "    word_len = 1 if len(word) <= 4 else 2 #se Ã¨ minore di 4 Ã¨ un articolo quasi sicuramente\n",
    "    exist = 1 if word in dictionary else 0 # esiste nel dizionario?\n",
    "    min_len = len(similar_word) > 4 #se Ã¨ piÃ¹ lungo di 4 evito che siano articoli\n",
    "    check_first_word = 1 if word[0] == similar_word[0] else 0 #controllo la prima lettera della parola\n",
    "    levenshtein_distance = nltk.edit_distance(word, similar_word) <= word_len #sostituire con quello di marta\n",
    "    weight = word_len + min_similarity + min_len + exist + levenshtein_distance\n",
    "    if weight > min_weight:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellChecker(word):\n",
    "    most_similar = model.wv.most_similar(word, topn=50) #parole simili dal modello\n",
    "    result = list(filter(lambda x: weight(word, x[0], x[1]), most_similar))#filtro con la funzione weight\n",
    "    result = {'prediction': result[:3]} #dizionario con le prime 5 parole\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-02 19:14:45,452 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-04-02 19:14:45,534 : INFO : precomputing L2-norms of ngram weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction': [('professore', 0.6842303276062012),\n",
       "  ('confessore', 0.6782220602035522),\n",
       "  ('professoressa', 0.5594971179962158)]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellChecker('prpfessore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('location', 0.0006342294),\n",
       " ('banca', 0.000510357),\n",
       " ('ricky', 0.00047233634),\n",
       " ('manager', 0.00042126453),\n",
       " ('sanpaolo', 0.00041730888),\n",
       " ('salvare', 0.0003768027),\n",
       " ('sam', 0.00036531643),\n",
       " ('ramses', 0.00036193614),\n",
       " ('lazio', 0.00035225626),\n",
       " ('quota', 0.00034832882),\n",
       " ('torna', 0.00033939298),\n",
       " ('gruppo', 0.0003303498),\n",
       " ('riesce', 0.00033010312),\n",
       " ('controller', 0.0003288965),\n",
       " ('dovrÃ ', 0.000321276),\n",
       " ('far', 0.00031655602),\n",
       " ('fare', 0.0003151168),\n",
       " ('salvezza', 0.0003143854),\n",
       " ('posto', 0.0003118625),\n",
       " ('intesa', 0.00029188063),\n",
       " ('milioni', 0.0002914475),\n",
       " ('comincia', 0.00029035666),\n",
       " ('uscire', 0.0002885761),\n",
       " ('horowitz', 0.0002813761),\n",
       " ('fa', 0.00027646072),\n",
       " ('lester', 0.00027384903),\n",
       " ('parco', 0.0002697703),\n",
       " ('ligure', 0.00026862972),\n",
       " ('prussia', 0.00026022564),\n",
       " ('picchiata', 0.00025876067),\n",
       " ('allenatore', 0.0002582462),\n",
       " ('newman', 0.0002566271),\n",
       " ('qui', 0.0002476692),\n",
       " ('girone', 0.00024693823),\n",
       " ('creditore', 0.00023616463),\n",
       " ('portarsi', 0.00023424067),\n",
       " ('stegosaurus', 0.00022968795),\n",
       " ('nuova', 0.00022926189),\n",
       " ('diavoli', 0.00022869272),\n",
       " ('manovra', 0.00022641922),\n",
       " ('mosÃ¨', 0.00022548638),\n",
       " ('napoleone', 0.00022522846),\n",
       " ('garibaldi', 0.00022147254),\n",
       " ('vai', 0.00022083259),\n",
       " ('promozione', 0.00022061348),\n",
       " ('album', 0.00022019097),\n",
       " ('bambina', 0.00021873596),\n",
       " ('guida', 0.00021835689),\n",
       " ('b', 0.00021629804),\n",
       " ('sta', 0.00021531811)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_output_word(model, ['devi', 'salire', 'in'], topn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
